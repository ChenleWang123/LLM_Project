import os
import pandas as pd
import torch
import ast
import os
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from tqdm import tqdm

# ================= é…ç½®åŒºåŸŸ =================
# æ¨¡å‹è·¯å¾„ (é¦–æ¬¡è¿è¡Œä¼šè‡ªåŠ¨ä¸‹è½½)
MODEL_ID = "mistralai/Mistral-7B-Instruct-v0.2"
# æ–‡ä»¶è·¯å¾„ (ç¡®ä¿è¿™4ä¸ªcsvæ–‡ä»¶å’Œè„šæœ¬åœ¨åŒä¸€ç›®å½•)
FILES = {
    "train_mcq": "data/train_dataset_mcq.csv",
    "test_mcq": "data/test_dataset_mcq.csv",
    "train_saq": "data/train_dataset_saq.csv",
    "test_saq": "data/test_dataset_saq.csv"
}
# ===========================================

def setup_model():
    print(f"Loading model: {MODEL_ID}...")

    # 4-bit é‡åŒ–é…ç½® (ä¸ºäº†åœ¨ HPC çœæ˜¾å­˜å¹¶æé€Ÿ)
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.float16,
        bnb_4bit_use_double_quant=True,
    )

    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_ID,
        quantization_config=bnb_config,
        device_map="auto",
        trust_remote_code=True
    )
    return tokenizer, model

def run_mcq(model, tokenizer):
    print("\n--- Running MCQ Task (Logits Strategy) ---")
    df_test = pd.read_csv(FILES["test_mcq"])

    # é¢„è®¡ç®— A, B, C, D çš„ token ID
    choices = ["A", "B", "C", "D"]
    choice_ids = [tokenizer.encode(c, add_special_tokens=False)[-1] for c in choices]

    results = []

    for _, row in tqdm(df_test.iterrows(), total=len(df_test)):
        # æ„é€  Prompt: åªè¦é—®é¢˜å’Œé€‰é¡¹
        prompt = f"[INST] Read the question. Choose the correct option (A, B, C, or D).\n\n{row['prompt']} [/INST] The answer is"

        inputs = tokenizer(prompt, return_tensors="pt").to("cuda")

        with torch.no_grad():
            outputs = model(**inputs)
            # è·å–æœ€åä¸€ä¸ª token çš„é¢„æµ‹æ¦‚ç‡åˆ†å¸ƒ
            logits = outputs.logits[0, -1, :]

            # åªæ¯”è¾ƒ A, B, C, D å››ä¸ª token çš„åˆ†æ•°
            scores = [logits[i].item() for i in choice_ids]
            best_idx = scores.index(max(scores)) # æ‰¾åˆ°åˆ†æ•°æœ€é«˜çš„ç´¢å¼•
            best_choice = choices[best_idx]

        # æ ¼å¼åŒ–è¾“å‡º (True/False)
        entry = {
            "MCQID": row['MCQID'],
            "A": False, "B": False, "C": False, "D": False
        }
        entry[best_choice] = True
        results.append(entry)

    # ä¿å­˜ç»“æœ
    out_df = pd.DataFrame(results)
    out_df = out_df[["MCQID", "A", "B", "C", "D"]] # ç¡®ä¿åˆ—é¡ºåº
    out_df.to_csv("mcq_prediction.tsv", sep='\t', index=False)
    print("âœ… MCQ predictions saved to mcq_prediction.tsv")

def run_saq(model, tokenizer):
    print("\n--- Running SAQ Task (Few-Shot Strategy) ---")
    df_train = pd.read_csv(FILES["train_saq"])
    df_test = pd.read_csv(FILES["test_saq"])

    # 1. å‡†å¤‡ Few-Shot æ ·æœ¬ (ä»è®­ç»ƒé›†æå–æ­£ç¡®ç­”æ¡ˆ)
    def get_clean_ans(s):
        try:
            d = ast.literal_eval(s)
            return d[0]['en_answers'][0] if d and 'en_answers' in d[0] else "unknown"
        except: return "unknown"

    df_train['clean_ans'] = df_train['annotations'].apply(get_clean_ans)

    # éšæœºå– 3 ä¸ªä¾‹å­ä½œä¸ºç¤ºèŒƒ
    examples = df_train.sample(3)
    few_shot_prompt = ""
    for _, row in examples.iterrows():
        few_shot_prompt += f"Question: {row['en_question']}\nAnswer: {row['clean_ans']}\n\n"

    results = []

    for _, row in tqdm(df_test.iterrows(), total=len(df_test)):
        question = row['en_question']

        # 2. æ„é€  Prompt: å¼ºæŒ‡ä»¤ + ä¾‹å­ + å½“å‰é—®é¢˜
        instruction = "Answer the question with a single entity, phrase, or number. Do not use full sentences."
        final_prompt = f"[INST] {instruction}\n\n{few_shot_prompt}Question: {question}\nAnswer: [/INST]"

        inputs = tokenizer(final_prompt, return_tensors="pt").to("cuda")

        # 3. ç”Ÿæˆ (é™åˆ¶é•¿åº¦ï¼Œé˜²æ­¢åºŸè¯)
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=15, # å…³é”®ï¼šé™åˆ¶åªèƒ½è¾“å‡ºçŸ­è¯­
                pad_token_id=tokenizer.eos_token_id,
                do_sample=False # è´ªå©ªæœç´¢ï¼Œæœ€ç¨³å®š
            )

        ans = tokenizer.decode(outputs[0], skip_special_tokens=True)
        # æå– [/INST] ä¹‹åçš„å†…å®¹
        ans = ans.split("[/INST]")[-1].strip().split("\n")[0]

        results.append({"ID": row['ID'], "answer": ans})

    # ä¿å­˜ç»“æœ
    out_df = pd.DataFrame(results)
    out_df.to_csv("saq_prediction.tsv", sep='\t', index=False)
    print("âœ… SAQ predictions saved to saq_prediction.tsv")

if __name__ == "__main__":
    if not torch.cuda.is_available():
        print("âŒ Error: GPU not found. This script requires a GPU.")
    else:
        tokenizer, model = setup_model()
        run_mcq(model, tokenizer)
        run_saq(model, tokenizer)
        print("\nğŸ‰ All tasks completed! Zip the .tsv files and submit.")